
  0%|                                                                                                        | 0/500 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
> /data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py(2650)training_step()
   2649         import ipdb; ipdb.set_trace()
-> 2650         if is_sagemaker_mp_enabled():
   2651             loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)
{'input_ids': tensor([[    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   474,  1348,   372,   471,   263,  2217,
          4688,   760,   310,   278, 11801,    13,  2277, 29937,  4007, 22137,
         29901,   474,  4658,   372,   471,  4688,   297,   278, 11801,     2,
             2,     2,     2,     2,     2],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   896, 11097,   825, 29879,  1900,   363,
          1009,  3942,    13,  2277, 29937,  4007, 22137, 29901,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,     2,     2,
             2,     2,     2,     2,     2],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   372,  7424,   304,   367,   263, 12176,
          1108,   411,   534,  2707, 18563,   773,  5883,  3174,    13,  2277,
         29937,  4007, 22137, 29901,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   297, 14721,   727,   263,  3287,   310,
         10609,   373, 20346,  3081,    13,  2277, 29937,  4007, 22137, 29901,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,     2,
             2,     2,     2,     2,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,   474,  4658,   372,   471,  4688,   297,   278, 11801,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,  -100,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,  -100,
          -100,  -100,  -100,  -100,  -100]], device='cuda:0')}
*** AttributeError
*** AttributeError
<class 'transformers.tokenization_utils_base.BatchEncoding'>
<bound method BatchEncoding.tokens of {'input_ids': tensor([[    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   474,  1348,   372,   471,   263,  2217,
          4688,   760,   310,   278, 11801,    13,  2277, 29937,  4007, 22137,
         29901,   474,  4658,   372,   471,  4688,   297,   278, 11801,     2,
             2,     2,     2,     2,     2],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   896, 11097,   825, 29879,  1900,   363,
          1009,  3942,    13,  2277, 29937,  4007, 22137, 29901,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,     2,     2,
             2,     2,     2,     2,     2],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   372,  7424,   304,   367,   263, 12176,
          1108,   411,   534,  2707, 18563,   773,  5883,  3174,    13,  2277,
         29937,  4007, 22137, 29901,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   297, 14721,   727,   263,  3287,   310,
         10609,   373, 20346,  3081,    13,  2277, 29937,  4007, 22137, 29901,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,     2,
             2,     2,     2,     2,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,   474,  4658,   372,   471,  4688,   297,   278, 11801,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,  -100,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,  -100,
          -100,  -100,  -100,  -100,  -100]], device='cuda:0')}>
*** AttributeError
tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,   474,  4658,   372,   471,  4688,   297,   278, 11801,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,  -100,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,  -100,
          -100,  -100,  -100,  -100,  -100]], device='cuda:0')
tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,   474,  4658,   372,   471,  4688,   297,   278, 11801,  -100,
         -100,  -100,  -100,  -100,  -100], device='cuda:0')
<class 'transformers.tokenization_utils_base.BatchEncoding'>
*** AttributeError: 'LlamaForCausalLM' object has no attribute 'tokenizer'
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096, padding_idx=0)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear4bit(
                in_features=4096, out_features=4096, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
              (v_proj): Linear4bit(
                in_features=4096, out_features=4096, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)
LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)
*** ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
*** OverflowError: out of range integral type conversion attempted
tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,   474,  4658,   372,   471,  4688,   297,   278, 11801,  -100,
         -100,  -100,  -100,  -100,  -100], device='cuda:0')
*** TypeError: 'method' object is not subscriptable
*** ValueError: tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).
*** ValueError: tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).
*** ValueError: tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).
*** ValueError: tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).
'i believe it was early in the summer'
<bound method BatchEncoding.keys of {'input_ids': tensor([[    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   474,  1348,   372,   471,   263,  2217,
          4688,   760,   310,   278, 11801,    13,  2277, 29937,  4007, 22137,
         29901,   474,  4658,   372,   471,  4688,   297,   278, 11801,     2,
             2,     2,     2,     2,     2],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   896, 11097,   825, 29879,  1900,   363,
          1009,  3942,    13,  2277, 29937,  4007, 22137, 29901,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,     2,     2,
             2,     2,     2,     2,     2],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   372,  7424,   304,   367,   263, 12176,
          1108,   411,   534,  2707, 18563,   773,  5883,  3174,    13,  2277,
         29937,  4007, 22137, 29901,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
          5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
           694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
         29878,   448, 29945, 29901,   297, 14721,   727,   263,  3287,   310,
         10609,   373, 20346,  3081,    13,  2277, 29937,  4007, 22137, 29901,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,     2,
             2,     2,     2,     2,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,   474,  4658,   372,   471,  4688,   297,   278, 11801,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   896, 11097,
           825, 29879,  1900,   363,   963,   322,  1009,  3942,  -100,  -100,
          -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,   967,  7424,   304,   367,   263,  4802,
          1108,   411,   534,  2707, 18563],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
           727,   338,   263,  3287,   310, 26307,   373, 20346,  3081,  -100,
          -100,  -100,  -100,  -100,  -100]], device='cuda:0')}>
dict_keys(['input_ids', 'attention_mask', 'labels'])
tensor([    1,   835, 12968, 29901,  1152,   278,  2183,  1881, 10541, 29892,
         5706,   385, 13052,  1821,   610,   481,  1092,   559,   363,   263,
          694, 13344,  5177,   411,   289,   370,   569, 11462,   472,  5807,
        29878,   448, 29945, 29901,   474,  1348,   372,   471,   263,  2217,
         4688,   760,   310,   278, 11801,    13,  2277, 29937,  4007, 22137,
        29901,   474,  4658,   372,   471,  4688,   297,   278, 11801,     2,
            2,     2,     2,     2,     2], device='cuda:0')
'<s> ### Human: For the given input sentence, generate an intelligible paraphrase for a noisy environment with babble noise at snr -5: i think it was a little early part of the summer\n### Assistant: i believe it was early in the summer</s></s></s></s></s></s>'
*** SyntaxError: unterminated string literal (detected at line 1)
*** NameError: name 'attention_mask' is not defined
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')
> /data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py(2654)training_step()
   2653
-> 2654         with self.compute_loss_context_manager():
   2655             loss = self.compute_loss(model, inputs)
> /data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py(2655)training_step()
   2654         with self.compute_loss_context_manager():
-> 2655             loss = self.compute_loss(model, inputs)
   2656
> /data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py(2654)training_step()
   2653
-> 2654         with self.compute_loss_context_manager():
   2655             loss = self.compute_loss(model, inputs)
tensor(2.7425, device='cuda:0', grad_fn=<NllLossBackward0>)
1
> /data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py(2650)training_step()
   2649         import ipdb; ipdb.set_trace()
-> 2650         if is_sagemaker_mp_enabled():
   2651             loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)
Error in sys.excepthook:
Traceback (most recent call last):
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/IPython/core/debugger.py", line 145, in BdbQuit_excepthook
    raise ValueError(
ValueError: `BdbQuit_excepthook` is deprecated since version 5.1
Original exception was:
Traceback (most recent call last):
  File "/nethome/achingacham/PycharmProjects/LLaMA/scripts/finetune_llama.py", line 156, in <module>
    trainer.train() #import inspect; inspect.getfile(trainer.train)
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 1809, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2650, in training_step
    if is_sagemaker_mp_enabled():
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py", line 2650, in training_step
    if is_sagemaker_mp_enabled():
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/data/users/achingacham/anaconda3/envs/llama/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit